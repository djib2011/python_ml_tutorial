{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "In this tutorial we'll learn how we can process natural language documents into a form understandable by computers. \n",
    "\n",
    "Up till now, we've only seen **structured data**. Structured data, are data that are organized in a known *schema*. We know what features there are and in most cases what each feature corresponds to. Some examples of structured data are DataFrames, spreadsheets and relational databases.\n",
    "\n",
    "In contrast, natural language is, by its nature, **unstructured**, meaning that we don't know the *schema* of the data. Finding a *structure* that describes the data is necessary in order to feed them into a ML model. This will be our main goal during this tutorial.\n",
    "\n",
    "For NLP we will be using python's [Natural Language Toolkit](http://www.nltk.org/), as well as several functions from scikit-learn. More information about the library's functionality can be found in the [NLTK book](http://www.nltk.org/book/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NTLK is a vast library that includes multiple sub-modules. When installed, NLTK is striped down to its most basic functionality. To access the more complex sub-modules, we need to manually download each one. This can be done through `nltk.download()`. We'll see how this is used in practice.\n",
    "\n",
    "## Loading corpora\n",
    "\n",
    "There are three main ways of loading a document corpus in python:\n",
    "\n",
    "- **Download through a python library**.  \n",
    "If we want to download a well-known dataset, chances are we can do it through a library like scikit-learn or nltk. For instance, the [reuters dataset](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection), can be retrieved through ntlk:\n",
    "\n",
    "```python\n",
    "nltk.download('reuters')  # download the dataset\n",
    "\n",
    "from nltk.corpus import reuters  # import it\n",
    "\n",
    "print('The dataset has a total of {} categories:'.format(len(reuters.categories())))\n",
    "print(reuters.categories()[:10])  # print first 10 category names\n",
    "print(reuters.fileids('category name'))  # print the file ids for a category, given its name \n",
    "```\n",
    "\n",
    "- **Download directly from the internet**.  \n",
    "If a document is available in the internet, we can retrieve using urllib:\n",
    "\n",
    "```python\n",
    "import urllib\n",
    "url = 'http://www.gutenberg.org/files/2554/2554-0.txt'  # document's url\n",
    "response = urllib.urlopen(url)  # send the request\n",
    "raw = response.read().decode('utf8')  # store the document in s tring\n",
    "print(raw[:500])  # print its 500 first characters\n",
    "```\n",
    "\n",
    "- **Load from a local file**.  \n",
    "If we've already downloaded the document in our computer, we'll just need to load it. We've seen how do so in a previous tutorial.\n",
    "\n",
    "```python\n",
    "with open('mydoc.txt', 'r') as f:  # open 'mydoc.txt' for reading\n",
    "    document = ''\n",
    "    for line in f:  # read each line of the text file one at a time\n",
    "        document += line  # store it to a string\n",
    "```\n",
    "\n",
    "## Typical NLP workflow\n",
    "\n",
    "The goal of any NLP workflow is to **remove** any **unnecessary content** from the document and **structure** it as best as possible.\n",
    "\n",
    "Suppose we've loaded a document using one of the previous methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"\n",
    "Commerce Secretary Malcolm Baldrige\n",
    "said he supported efforts to persuade newly-industrialized\n",
    "countries (NICS) to revalue currencies that are tied to the\n",
    "dollar in order to help the United States cut its massive trade\n",
    "deficit.\n",
    "    \"We do need to do something with those currencies or we\n",
    "will be substituting Japanese products for Taiwanese products,\"\n",
    "or those of other nations with currencies tied to the dollar,\n",
    "Baldrige told a House banking subcommittee.\n",
    "    The U.S. dollar has declined in value against the Yen and\n",
    "European currencies, but has changed very little against the\n",
    "currencies of some developing countries such as South Korea and\n",
    "Taiwan because they are linked to the value of the dollar.\n",
    "    As a result, efforts to reduce the value of the dollar over\n",
    "the past year and a half have done little to improve the trade\n",
    "deficits with those countries.\n",
    "    Baldrige told a House Banking subcommittee that the\n",
    "Treasury Department was attempting to persuade those countries\n",
    "to reach agreement with the United States on exchange rates.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String operations\n",
    "\n",
    "After loading the document, there are a few things we might want to do. First of all, we should convert all the characters to **lowercase**. This is done because the computer treats upper and lower case letters differently (e.g. *Cat* and *cat* are two totally different words for the computer). Afterwards, we'll remove all punctuation from the string and **split** it into words. The goal is to create a list of all the words appearing in the document.\n",
    "\n",
    "For the first task (i.e. converting the document to lowercase), we can use python's built-in string method `.lower()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "commerce secretary malcolm baldrige\n",
      "said he supported efforts to persuade newly-industrialized\n",
      "countries (nics) to revalue currencies that are tied to the\n",
      "dollar in order to help the united states cut its massive trade\n",
      "deficit.\n",
      "    \"we do need to do something with those currencies or we\n",
      "will be substituting japanese products for taiwanese products,\"\n",
      "or those of other nations with currencies tied to the dollar,\n",
      "baldrige told a house banking subcommittee.\n",
      "    the u.s. dollar has declined in value against the yen and\n",
      "european currencies, but has changed very little against the\n",
      "currencies of some developing countries such as south korea and\n",
      "taiwan because they are linked to the value of the dollar.\n",
      "    as a result, efforts to reduce the value of the dollar over\n",
      "the past year and a half have done little to improve the trade\n",
      "deficits with those countries.\n",
      "    baldrige told a house banking subcommittee that the\n",
      "treasury department was attempting to persuade those countries\n",
      "to reach agreement with the united states on exchange rates.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document = document.lower()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second task, we'll use string method `.replace()` to remove each punctuation mark. Instead of identifying them manually, they are available in a package called *string*, stored in a variable called `punctuation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "\n",
      "commerce secretary malcolm baldrige\n",
      "said he supported efforts to persuade newlyindustrialized\n",
      "countries nics to revalue currencies that are tied to the\n",
      "dollar in order to help the united states cut its massive trade\n",
      "deficit\n",
      "    we do need to do something with those currencies or we\n",
      "will be substituting japanese products for taiwanese products\n",
      "or those of other nations with currencies tied to the dollar\n",
      "baldrige told a house banking subcommittee\n",
      "    the us dollar has declined in value against the yen and\n",
      "european currencies but has changed very little against the\n",
      "currencies of some developing countries such as south korea and\n",
      "taiwan because they are linked to the value of the dollar\n",
      "    as a result efforts to reduce the value of the dollar over\n",
      "the past year and a half have done little to improve the trade\n",
      "deficits with those countries\n",
      "    baldrige told a house banking subcommittee that the\n",
      "treasury department was attempting to persuade those countries\n",
      "to reach agreement with the united states on exchange rates\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)  # string containing all available punctuation marks\n",
    "\n",
    "for punct_mark in string.punctuation:  # iterate over all punctuation marks\n",
    "    document = document.replace(punct_mark, '')  # remove each punctuation mark\n",
    "    \n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the task of splitting the document into words, one thought could be to use the string method `.split()`. However, besides spaces (`' '`), we need to separate tabs (`\\t`), new lines (`\\n`), etc. An easier way is available through nltk.\n",
    "\n",
    "Because words are typically referred to as **tokens**, this process is called **tokenization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['commerce', 'secretary', 'malcolm', 'baldrige', 'said', 'he', 'supported', 'efforts', 'to', 'persuade', 'newlyindustrialized', 'countries', 'nics', 'to', 'revalue', 'currencies', 'that', 'are', 'tied', 'to', 'the', 'dollar', 'in', 'order', 'to', 'help', 'the', 'united', 'states', 'cut', 'its', 'massive', 'trade', 'deficit', 'we', 'do', 'need', 'to', 'do', 'something', 'with', 'those', 'currencies', 'or', 'we', 'will', 'be', 'substituting', 'japanese', 'products', 'for', 'taiwanese', 'products', 'or', 'those', 'of', 'other', 'nations', 'with', 'currencies', 'tied', 'to', 'the', 'dollar', 'baldrige', 'told', 'a', 'house', 'banking', 'subcommittee', 'the', 'us', 'dollar', 'has', 'declined', 'in', 'value', 'against', 'the', 'yen', 'and', 'european', 'currencies', 'but', 'has', 'changed', 'very', 'little', 'against', 'the', 'currencies', 'of', 'some', 'developing', 'countries', 'such', 'as', 'south', 'korea', 'and', 'taiwan', 'because', 'they', 'are', 'linked', 'to', 'the', 'value', 'of', 'the', 'dollar', 'as', 'a', 'result', 'efforts', 'to', 'reduce', 'the', 'value', 'of', 'the', 'dollar', 'over', 'the', 'past', 'year', 'and', 'a', 'half', 'have', 'done', 'little', 'to', 'improve', 'the', 'trade', 'deficits', 'with', 'those', 'countries', 'baldrige', 'told', 'a', 'house', 'banking', 'subcommittee', 'that', 'the', 'treasury', 'department', 'was', 'attempting', 'to', 'persuade', 'those', 'countries', 'to', 'reach', 'agreement', 'with', 'the', 'united', 'states', 'on', 'exchange', 'rates']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') # required for the tokenizer\n",
    "\n",
    "words = nltk.word_tokenize(document)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword removal\n",
    "\n",
    "The next step involves reducing the number of tokens, with a minimal loss of the semantic content of the document. Why this is necessary will become apparent later on. The easiest thing to do is to remove the **stopwords** (i.e. common words that exist in almost every document and don't contribute to its semantic content). Some examples of stopwords are *\"and\"*, *\"this\"*, *\"that\"*, *\"me\"*, etc.\n",
    "\n",
    "This can be easily done again through nltk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['commerce', 'secretary', 'malcolm', 'baldrige', 'said', 'supported', 'efforts', 'persuade', 'newlyindustrialized', 'countries', 'nics', 'revalue', 'currencies', 'tied', 'dollar', 'order', 'help', 'united', 'states', 'cut', 'massive', 'trade', 'deficit', 'need', 'something', 'currencies', 'substituting', 'japanese', 'products', 'taiwanese', 'products', 'nations', 'currencies', 'tied', 'dollar', 'baldrige', 'told', 'house', 'banking', 'subcommittee', 'us', 'dollar', 'declined', 'value', 'yen', 'european', 'currencies', 'changed', 'little', 'currencies', 'developing', 'countries', 'south', 'korea', 'taiwan', 'linked', 'value', 'dollar', 'result', 'efforts', 'reduce', 'value', 'dollar', 'past', 'year', 'half', 'done', 'little', 'improve', 'trade', 'deficits', 'countries', 'baldrige', 'told', 'house', 'banking', 'subcommittee', 'treasury', 'department', 'attempting', 'persuade', 'countries', 'reach', 'agreement', 'united', 'states', 'exchange', 'rates']\n",
      "length before stopword removal: 166\n",
      "length after stopword removal: 88\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')  # download a file containing english stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "filtered_words = [word for word in words if word not in stopwords.words('english')]  # filter out stopwords\n",
    "\n",
    "print(filtered_words)\n",
    "print('length before stopword removal:', len(words))\n",
    "print('length after stopword removal:', len(filtered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we cut the size of the document by around half, without removing any of the words that define its meaning.\n",
    "\n",
    "## Stemming & Lemmatization\n",
    "\n",
    "Words in a document may appear in different lexical forms (e.g. *play*, *plays*, *playing*, *played*). While we, as humans, understand that all of these words relay the same meaning, the computer recognizes them as different. To resolve this, there are a couple of techniques (i.e. stemming and lemmatization) that aim to reduce a word to its inflectional form.\n",
    "\n",
    "**Stemming** refers to the process of reducing words to their *stem*. This is most commonly done by cutting their suffixes.\n",
    "\n",
    "**Lemmatization** is a procedure where a word is reduced to its *lemma* (or dictionary form). \n",
    "\n",
    "Both have a similar goal, but approach it differently. Stemming is usually the crude heuristic of chopping off parts of the word, while lemmatization uses a dictionary and attempts to perform a morphological analysis of the word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\thano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\thano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "\n",
      "Original             Stemmed              Lemmatized          \n",
      "------------------------------------------------------------\n",
      "commerce             commerc              commerce            \n",
      "secretary            secretari            secretary           \n",
      "malcolm              malcolm              malcolm             \n",
      "baldrige             baldrig              baldrige            \n",
      "said                 said                 said                \n",
      "supported            support              supported           \n",
      "efforts              effort               effort              \n",
      "persuade             persuad              persuade            \n",
      "newlyindustrialized  newlyindustri        newlyindustrialized \n",
      "countries            countri              country             \n",
      "nics                 nic                  nics                \n",
      "revalue              revalu               revalue             \n",
      "currencies           currenc              currency            \n",
      "tied                 tie                  tied                \n",
      "dollar               dollar               dollar              \n",
      "order                order                order               \n",
      "help                 help                 help                \n",
      "united               unit                 united              \n",
      "states               state                state               \n",
      "cut                  cut                  cut                 \n",
      "massive              massiv               massive             \n",
      "trade                trade                trade               \n",
      "deficit              deficit              deficit             \n",
      "need                 need                 need                \n",
      "something            someth               something           \n",
      "currencies           currenc              currency            \n",
      "substituting         substitut            substituting        \n",
      "japanese             japanes              japanese            \n",
      "products             product              product             \n",
      "taiwanese            taiwanes             taiwanese           \n",
      "products             product              product             \n",
      "nations              nation               nation              \n",
      "currencies           currenc              currency            \n",
      "tied                 tie                  tied                \n",
      "dollar               dollar               dollar              \n",
      "baldrige             baldrig              baldrige            \n",
      "told                 told                 told                \n",
      "house                hous                 house               \n",
      "banking              bank                 banking             \n",
      "subcommittee         subcommitte          subcommittee        \n",
      "us                   us                   u                   \n",
      "dollar               dollar               dollar              \n",
      "declined             declin               declined            \n",
      "value                valu                 value               \n",
      "yen                  yen                  yen                 \n",
      "european             european             european            \n",
      "currencies           currenc              currency            \n",
      "changed              chang                changed             \n",
      "little               littl                little              \n",
      "currencies           currenc              currency            \n",
      "developing           develop              developing          \n",
      "countries            countri              country             \n",
      "south                south                south               \n",
      "korea                korea                korea               \n",
      "taiwan               taiwan               taiwan              \n",
      "linked               link                 linked              \n",
      "value                valu                 value               \n",
      "dollar               dollar               dollar              \n",
      "result               result               result              \n",
      "efforts              effort               effort              \n",
      "reduce               reduc                reduce              \n",
      "value                valu                 value               \n",
      "dollar               dollar               dollar              \n",
      "past                 past                 past                \n",
      "year                 year                 year                \n",
      "half                 half                 half                \n",
      "done                 done                 done                \n",
      "little               littl                little              \n",
      "improve              improv               improve             \n",
      "trade                trade                trade               \n",
      "deficits             deficit              deficit             \n",
      "countries            countri              country             \n",
      "baldrige             baldrig              baldrige            \n",
      "told                 told                 told                \n",
      "house                hous                 house               \n",
      "banking              bank                 banking             \n",
      "subcommittee         subcommitte          subcommittee        \n",
      "treasury             treasuri             treasury            \n",
      "department           depart               department          \n",
      "attempting           attempt              attempting          \n",
      "persuade             persuad              persuade            \n",
      "countries            countri              country             \n",
      "reach                reach                reach               \n",
      "agreement            agreement            agreement           \n",
      "united               unit                 united              \n",
      "states               state                state               \n",
      "exchange             exchang              exchange            \n",
      "rates                rate                 rate                \n"
     ]
    }
   ],
   "source": [
    "# required downloads for the stemmer/lemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('rslp')\n",
    "\n",
    "# we'll be using the wordnet lemmatizer and the porter stemmer\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# apply stemming/lemmatization to each word in the document\n",
    "lem_words = [wordnet_lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "stem_words = [porter_stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "# print results for comparison\n",
    "print('\\n{:<20} {:<20} {:<20}'.format('Original', 'Stemmed', 'Lemmatized'))\n",
    "print('-'*60)\n",
    "for i in range(len(filtered_words)):\n",
    "    print('{:<20} {:<20} {:<20}'.format(filtered_words[i], stem_words[i], lem_words[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to try out different stemmers or lemmatizers and select whichever you prefer.\n",
    "\n",
    "Once we have completed all linguistic pre-processing steps, we can move on to the next task: to structure the input (i.e. to convert the document to a form recognizable by ML algorithms). The algorithms we've seen up till now could only handle **vectors** (i.e. a series of numbers - each corresponding to the value of a specific feature) as input. Each vector will be derived from the terms of a document and should be able to *describe* it. In this sense, documents with similar subjects should have vectors close to one another. Likewise, unrelated documents' vectors should be far apart.\n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "Bag-of-words is probably the simplest method of *vectorizing* documents. First, it finds all unique terms in a corpus of documents; think of this as a list of the *features* of the (where each term is a feature). Then, for each document, it counts the number appearances and forms a vector from the unique terms of the first step. \n",
    "\n",
    "Like we did in previous tutorials, we won't be using scikit-learn's implementation, to better understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'four': 4, 'three': 3, 'two': 2, 'one': 1})\n"
     ]
    }
   ],
   "source": [
    "doc = 'four three two one four two three four three four' # a random string\n",
    "\n",
    "from collections import Counter # Counter pretty much does all the work\n",
    "\n",
    "cnt = Counter(doc.split())\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fully understand how bag-of-words works, we need a corpus of documents. Let's create such, and pre-process them using the steps we saw previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lionel', 'messi', 'best', 'footbal', 'player', 'world', 'messi', 'play', 'barcelona', 'footbal', 'club', 'barcelona', 'footbal', 'club', 'play', 'spanish', 'primera', 'divis']\n",
      "['lionel', 'messi', 'footbal', 'player', 'play', 'barcelona', 'footbal', 'club', 'spanish', 'footbal', 'team']\n",
      "['barcelona', 'citi', 'northern', 'spanish', 'provinc', 'call', 'catalonia', 'largest', 'citi', 'catalonia', 'second', 'popul', 'spanish', 'citi']\n",
      "['python', 'program', 'languag', 'python', 'objectori', 'program', 'languag', 'unlik', 'cobol', 'python', 'interpret', 'program', 'languag']\n",
      "['cobol', 'compil', 'comput', 'program', 'languag', 'design', 'busi', 'use', 'program', 'languag', 'imper', 'procedur', 'sinc', '2002', 'objectori', 'python', 'better']\n"
     ]
    }
   ],
   "source": [
    "# Our new corpus\n",
    "documents = [\"Lionel Messi is the best football player in the world! Messi plays for Barcelona Football Club. Barcelona Football Club plays in the Spanish Primera Division.\",\n",
    "             \"Lionel Messi a football player, playing for Barcelona Football Club, a Spanish football team.\", \n",
    "             \"Barcelona is a city in a northern spanish province called Catalonia. It is the largest city in Catalonia and the second most populated spanish city.\", \n",
    "             \"Python is a programming language. Python is an object-oriented programming language. Unlike COBOL, Python is a interpreted programming language.\", \n",
    "             \"COBOL is a compiled computer programming language designed for business use. This programming language is imperative, procedural and, since 2002, object-oriented. But Python is better.\"]\n",
    "\n",
    "# Pre-process the documents\n",
    "def preprocess_document(document):\n",
    "    # function that performs all linguistic preprocessing steps mentioned previously\n",
    "    document = document.lower()\n",
    "    for punct_mark in string.punctuation:\n",
    "        document = document.replace(punct_mark, '')\n",
    "    words = nltk.word_tokenize(document.lower())\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    stemmed_words = [porter_stemmer.stem(word) for word in filtered_words]\n",
    "    return stemmed_words\n",
    "\n",
    "preprocessed_documents = [preprocess_document(doc) for doc in documents]\n",
    "\n",
    "for doc in preprocessed_documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create a list of all unique terms in our corpus. This is called a **vocabulary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'footbal': 6, 'program': 5, 'languag': 5, 'barcelona': 4, 'spanish': 4, 'python': 4, 'messi': 3, 'play': 3, 'club': 3, 'citi': 3, 'lionel': 2, 'player': 2, 'catalonia': 2, 'objectori': 2, 'cobol': 2, 'best': 1, 'world': 1, 'primera': 1, 'divis': 1, 'team': 1, 'northern': 1, 'provinc': 1, 'call': 1, 'largest': 1, 'second': 1, 'popul': 1, 'unlik': 1, 'interpret': 1, 'compil': 1, 'comput': 1, 'design': 1, 'busi': 1, 'use': 1, 'imper': 1, 'procedur': 1, 'sinc': 1, '2002': 1, 'better': 1})\n"
     ]
    }
   ],
   "source": [
    "total_counter = Counter(preprocessed_documents[0])\n",
    "\n",
    "for i in range(1, len(preprocessed_documents)):\n",
    "    total_counter += Counter(preprocessed_documents[i])\n",
    "\n",
    "print(total_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above depicts our corpus' vocabulary along with the total number of appearances of each term. Terms appearing only **once** in the whole corpus aren't much useful, as they can't be used see how two documents are similar. In order to reduce the size of the vocabulary we might want to remove terms that appear very few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lionel', 'messi', 'footbal', 'player', 'play', 'barcelona', 'club', 'spanish', 'citi', 'catalonia', 'python', 'program', 'languag', 'objectori', 'cobol']\n"
     ]
    }
   ],
   "source": [
    "threshold = 1\n",
    "\n",
    "vocabulary = [word for word in total_counter if total_counter[word] > threshold]  # remove rarely occurring words\n",
    "\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, terms that appear in almost **every** document aren't very useful and could be removed as well.\n",
    "\n",
    "We will now complete our bag-of-words model by counting the appearance of each term in every document and placing them all in an array where each line would represent a single document and each row a term. An example of such an array with $N$ documents and $M$ terms, is the following:\n",
    "\n",
    " | $t_1$ | $t_2$ | ... | $t_M$  \n",
    ":-: | :-: | :-: | :-: | :-:  \n",
    "$d_1$ | 1 | 0 | ... | 2  \n",
    "$d_2$ | 0 | 0 | ... | 3\n",
    "... | ... | ... | ... | ...\n",
    "$d_N$ | 0 | 4 | ... | 0\n",
    "\n",
    "The number in the position **$(i, j)$** of the array represents **how many times term $t_j$ appears in document $d_i$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lionel', 'messi', 'footbal', 'player', 'play', 'barcelona', 'club', 'spanish', 'citi', 'catalonia', 'python', 'program', 'languag', 'objectori', 'cobol'] \n",
      "\n",
      "[[1. 2. 3. 1. 2. 2. 2. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 3. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 2. 3. 2. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 3. 3. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 2. 2. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "bag_array = np.zeros((len(preprocessed_documents), len(vocabulary)))  # create an N-by-M arrays\n",
    "\n",
    "document_counters = [Counter(doc) for doc in preprocessed_documents]  # count terms in each document\n",
    "\n",
    "for i in range(len(preprocessed_documents)):\n",
    "    for j in range(len(vocabulary)):\n",
    "        bag_array[i,j] += document_counters[i][vocabulary[j]]  # populate the array\n",
    "\n",
    "print(vocabulary, '\\n')\n",
    "print(bag_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've finally reached a point where we have an array we can feed into a ML algorithm! The above array would typically serve as our input array $X$ and depending on whether or not we have labels for each document, we would either have a supervised or an unsupervised problem. This array from this point on should be treated as any other input to a **structured** ML problem (e.g. we could shuffle it, split it into training/test sets, perform any feature scaling/selection/extraction, etc.).\n",
    "\n",
    "It's time to discuss a couple of interesting properties of problems involving NLP.\n",
    "\n",
    "1. The first thing to note is that input arrays when dealing with natural language tend to have a **high dimensionality**, especially when using models like bag-of-words, where they treat each unique term as a new feature. This is the reason why, during this tutorial, almost all our pre-processing operations had the goal of reducing the number of terms in the corpus. Think about it:\n",
    "  - While **lowercasing** our words had the goal of mapping two different words that had the same semantic meaning (e.g. *Cat* and *cat*) to a common term, it had the much desired effect of removing a lot of would-be terms (those that have uppercase characters in them) from the vocabulary.\n",
    "  - The same can be said for **stemming/lemmatization**.\n",
    "  - **Stopword removal** had the goal of removing several terms from the vocabulary, that are too common to convey any semantic information.\n",
    "  - Removing the **least-commonly appearing terms** also had the goal of reducing the vocabulary size.\n",
    "  - As would removing the **most-common terms**, which was discussed but not performed.\n",
    "2. Another interesting thing to note about bag-of-words arrays, are how **sparse** they are (i.e. they have a lot of zero elements). This becomes even more apparent in large corpora consisting of documents talking about multiple topics. This property should be taken into account when designing the rest of the ML workflow (pre-processing steps, algorithm, etc.).\n",
    "3. When counting the number of appearances of every term in each document, no type of scaling was performed regarding the size of the document. This would have the effect of *large* documents having higher values (in the bag-of-words array) than *smaller* ones, which could be an issue if the corpus contains documents with highly-variable in length. This third point will lead us into our next technique which aims at alleviating this issue. \n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "For a better weight assignment to each term, we'll use **TF-IDF** (Term Frequency - Inverse Document Frequency).\n",
    "\n",
    "As implied by its name, tf-idf consists of 2 terms. The first one is **Term Frequency (TF)**:\n",
    "\n",
    "$$ tf(t_j, d_i) = \\frac{f(t_j, d_i)}{\\sum_{t} f(t, d_i)}$$\n",
    "\n",
    "Where $t_j$ is a term in document $d_i$. *TF* essentially represents the frequency with which a term appears in a document. Terms with a large frequency as assigned a larger weight (and as a result are more important) than terms with a small frequency. This alleviates the problem of documents of varying length, as each term is *normalized* with the total number of terms in the document it appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14. 10.  8. 11.  7.] \n",
      "\n",
      "[[0.071 0.143 0.214 0.071 0.143 0.143 0.143 0.071 0.    0.    0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.1   0.1   0.3   0.1   0.1   0.1   0.1   0.1   0.    0.    0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.125 0.    0.25  0.375 0.25  0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.273 0.273\n",
      "  0.273 0.091 0.091]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.143 0.286\n",
      "  0.286 0.143 0.143]]\n"
     ]
    }
   ],
   "source": [
    "print(bag_array.sum(axis=1), '\\n')  # number of terms per document\n",
    "\n",
    "freq_array = np.zeros(shape=bag_array.shape)\n",
    "\n",
    "for i in range(len(bag_array)):\n",
    "    freq_array[i, :] = bag_array[i, :] / bag_array.sum(axis=1)[i] # term frequency\n",
    "                                                                  # (term appearances / number of terms in document)     \n",
    "                                                                    \n",
    "print(np.round(freq_array, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second term in tf-idf is the **Inverse Document Frequency**:\n",
    "\n",
    "$$ idf(t_j) = log \\left( \\frac{N}{df(t_j)} \\right)$$\n",
    "\n",
    "Where $N$ is the number of documents and $df(t_j)$ is the number of documents where term $t_j$ appears in. *IDF* is a measure of the **information** that each term conveys. If a term appears in every document, then that term's *IDF* would become equal to $0$ (the fraction would have a value of $1$). Likewise, if a term appears only in a few of the documents, its *IDF* would be relatively large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.398 0.398 0.398 0.398 0.398 0.222 0.398 0.222 0.699 0.699 0.398 0.398\n",
      " 0.398 0.398 0.398]\n"
     ]
    }
   ],
   "source": [
    "non_zero_elements_per_row = np.count_nonzero(bag_array, axis=0)  # np.count_zero counts how many non-zero elements an array has\n",
    "                                                                 # we are interested in counting this along each column\n",
    "\n",
    "idf = np.log10(float(len(bag_array))/non_zero_elements_per_row)  # the nominator is the number of documents \n",
    "                                                                 # (or the number of lines in 'bag_array')\n",
    "                                                                 # the denominator is the number of non-zero \n",
    "                                                                 # elements that each column of 'bag_array' has\n",
    "print(np.round(idf, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fully grasp how exactly the denominator is calculated, you can look at an equivalent code: \n",
    "\n",
    "```python\n",
    "non_zero_elements_per_row = np.zeros((len(bag_array[0])))\n",
    "\n",
    "for i in range(len(bag_array)):\n",
    "    for j in range(len(bag_array[0])):\n",
    "        if bag_array[i,j] > 0.0:\n",
    "            non_zero_elements_per_row[j] += 1\n",
    "```\n",
    "Finally, we can calculate tf-idf as the product of its two terms:\n",
    "\n",
    "$$tf{\\text -}idf(t_j, d_i) = tf(t_j, d_i) \\cdot idf(d_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.028 0.057 0.085 0.028 0.057 0.032 0.057 0.016 0.    0.    0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.04  0.04  0.119 0.04  0.04  0.022 0.04  0.022 0.    0.    0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.028 0.    0.055 0.262 0.175 0.    0.\n",
      "  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.109 0.109\n",
      "  0.109 0.036 0.036]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.057 0.114\n",
      "  0.114 0.057 0.057]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = freq_array * idf  # tf-idf is the product of tf with idf\n",
    "\n",
    "print(np.round(tf_idf, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to see if our model makes sense. \n",
    "\n",
    "We'll calculate the distances amongst each of the five documents, to see which ones are close to one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.12385863 0.79310084 0.75814618 0.75814618]\n",
      " [0.12385863 0.         0.79403179 0.76066177 0.76066177]\n",
      " [0.79310084 0.79403179 0.         0.91798954 0.91798954]\n",
      " [0.75814618 0.76066177 0.91798954 0.         0.10336104]\n",
      " [0.75814618 0.76066177 0.91798954 0.10336104 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "distances = np.zeros((len(tf_idf), len(tf_idf)))\n",
    "\n",
    "for i in range(len(tf_idf)):\n",
    "      for j in range(len(tf_idf)):\n",
    "            distances[i,j] = sum(np.abs(tf_idf[i] - tf_idf[j]))  # sum of absolute distance of the tf-idf vectors of each document\n",
    "\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the first two documents are close to one another, as their distance is small (around $0.12$). The same can be said for the last two documents (distance of around $0.1$). In contrast, all unrelated documents have a distance larger than $0.7$.\n",
    "\n",
    "The tf-idf array, like the bag-of-words array before, can be input in any ML workflow. \n",
    "\n",
    "We'll attempt to do so now. Because we don't have any labels available for the documents, we are forced to take an unsupervised approach. We'll use a k-means algorithm we discussed in the previous tutorial, which will aim at separating the documents into clusters. Since we are aware of the context of the documents, we can empirically evaluate the results. We'd expect the first two documents to end up in the same cluster and the last two in another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(3, random_state=99)\n",
    "km.fit_predict(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the first two documents ended up in the same cluster, as did the last two. The 3rd document which was unrelated got its own cluster. As a second example we'll see a clustering technique that doesn't require us to explicitly select the number of clusters.\n",
    "\n",
    "# Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering algorithms approach clustering differently to the centroid-based ones (like k-means). Their goal is to hierarchically cluster their input, with each cluster composed of sub-clusters, finally forming a tree-like structure.\n",
    "\n",
    "![](http://www.alglib.net/dataanalysis/i/clusters_ahc.png)\n",
    "\n",
    "There are two categories of hierarchical clustering algorithms:\n",
    "\n",
    "- **Agglomerative**, or bottom-up hierarchical clustering algorithms, start by assuming that each example is its own cluster and then begin merging clusters\n",
    "\n",
    "- **Divisive**, or top-down algorithms, start by assuming that every example belongs to a single cluster and then begin splitting this cluster into sub-clusters.\n",
    "\n",
    "Hierarchical clustering algorithms make use of the distances between the training examples to merge or split clusters. Unlike k-means, in hierarchical clustering algorithms, it isn't necessary to specify the number of clusters ($k$) a-priori.\n",
    "\n",
    "The algorithm we'll see, belongs to the first category and uses [Ward's minimum variance method](https://en.wikipedia.org/wiki/Ward%27s_method) for merging clusters together. This recursively looks for the pair of clusters, that when merged, will amount to the least increase to the total internal variance of the clusters. *(Note: with the term internal variance, we mean the variance of the the examples within a certain cluster. Total internal variance is the sum of the internal variances for all clusters.)*\n",
    "\n",
    "Initially, each example is considered to be one cluster. Then it looks for the pair of clusters that would lead to the least increase in total internal variance, if merged (i.e. Ward's method). Those two clusters are merged into one and the procedure is repeated until 2 clusters are left.\n",
    "\n",
    "The above can be simply implemented with [scikit-learn](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "Z = linkage(tf_idf, 'ward')  # train the algorithm\n",
    "\n",
    "dendrogram(Z)  # plots a dendrogram with the results of the clustering algorithm\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the documents that are close to one another, ended up in the same cluster. \n",
    "\n",
    "Let's try to apply what we've learned to a more complex example.\n",
    "\n",
    "## NLP/Clustering example\n",
    "\n",
    "For the example we'll use the [20 Newsgroups](http://qwone.com/~jason/20Newsgroups/) dataset, which is available through [sklearn](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the results to be better interpretable, we'll just include 3 categories from the dataset, each consisting of 5 documents. To make our lives easier, the categories will be rather distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: 15 \n",
      "\n",
      "In <16BA7103C3.I3150101@dbstu1.rz.tu-bs.de> I3150101@dbstu1.rz.tu-bs.de (Benedikt Rosenau) writes:\n",
      "\n",
      ">In article <1993Apr5.091258.11830@monu6.cc.monash.edu.au>\n",
      ">darice@yoyo.cc.monash.edu.au (Fred Rice) writes:\n",
      "> \n",
      ">(Deletion)\n",
      ">>>>Of course people say what they think to be the religion, and that this\n",
      ">>>>is not exactly the same coming from different people within the\n",
      ">>>>religion.  There is nothing with there existing different perspectives\n",
      ">>>>within the religion -- perhaps one can say that they t\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "categ = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball']\n",
    "\n",
    "data = reduce(lambda x,y: x+y, [fetch_20newsgroups(categories=[x], remove=('headers', 'footers'))['data'][:5] for x in categ])\n",
    "\n",
    "print('Input shape:', len(data), '\\n')\n",
    "print(data[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, instead of preprocessing the documents manually, we'll use scikit-learn's [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), which can support every [pre-processing](http://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes) we previously saw (stopwords, stemming, lemmatizing, ). We'll also use two extra parameters: `max_df=x`, which ignores any terms that appear in documents with a frequency larger than `x` (i.e. the most common terms) and `max_df=y`, which ignores terms that appear less than `y` times in the corpus (i.e. the least common terms). For this example we won't be performing any stemming/lemmatizing, but the vectorizer does support this functionality (see link above).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF array shape: (15, 124)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')  # remove terms appearing in more than 50% of the documents\n",
    "                                                                          # remove terms appearing less than 2 times in the corpus\n",
    "                                                                          # remove english stopwords\n",
    "\n",
    "tf_idf_array = vectorizer.fit_transform(data).toarray()  # returns a sparse matrix, to convert it to a regular array we use .toarray()\n",
    "\n",
    "print('TF-IDF array shape:', tf_idf_array.shape)\n",
    "\n",
    "Z = linkage(tf_idf_array, 'ward')\n",
    "\n",
    "labels = ['a'] * 5 + ['g'] * 5 + ['b'] * 5 # 'a' = atheism, 'g' = graphics, 'b' = baseball \n",
    "\n",
    "dendrogram(Z, labels=labels, color_threshold=0)\n",
    "\n",
    "colors = {'a': 'r', 'g': 'g', 'b': 'b'}\n",
    "\n",
    "for l in plt.gca().get_xticklabels():\n",
    "    l.set_color(colors[l.get_text()])\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm did a good enough job of placing similar documents in the same clusters, as we can see from above. Another thing to note is that hierarchical alrorithms are capable of identifying hierarchies within groups of similar documents. Also in a truly unsupervised setting the labels/colors in the x-axis would **not** be available.\n",
    "\n",
    "Let's try to do the same thing with **k-means**, this time with a few more documents per category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 300\n"
     ]
    }
   ],
   "source": [
    "data = reduce(lambda x,y: x+y, [fetch_20newsgroups(categories=[x], remove=('headers', 'footers'))['data'][:100] for x in categ])\n",
    "\n",
    "print('Total number of documents:', len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll run k-means for multiple values of $k$, like we did in the previous tutorial, to see if it can identify any distinct number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "tf_idf_array = vectorizer.fit_transform(data)  # vectorizer with the same parameters as before\n",
    "\n",
    "silhouette_scores = []\n",
    "for k in range(2, 10):\n",
    "    km = KMeans(k)\n",
    "    preds = km.fit_predict(tf_idf_array)\n",
    "    silhouette_scores.append(silhouette_score(tf_idf_array, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll plot the silhouette scores and identify the best $k$. This will represent the number of clusters our documents belong in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum average silhouette score for k = 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(2, 10), silhouette_scores, zorder=-1)\n",
    "best_k = np.argmax(silhouette_scores) + 2  # +2 because range() begins from k=2 and not 0\n",
    "plt.scatter(best_k, silhouette_scores[best_k-2], color='#ff7f0e') # for the same reason the best k is -2 from the list index\n",
    "plt.xlim([2,9])\n",
    "plt.annotate(\"best k\", xy=(best_k, silhouette_scores[best_k-2]), \n",
    "             xytext=(5, silhouette_scores[best_k-2]), arrowprops=dict(arrowstyle=\"->\"))  # add annotation\n",
    "print('Maximum average silhouette score for k =', best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the silhouette criterion we found 3 clusters in our dataset, which we know to be the correct number. \n",
    "\n",
    "Let's print the assignments made by k-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 1 2 1 2 2 1 2 2 2 2 2 2 1 2 2 2 1 0 2 2 1 1 2 2 2 1 2 1 2 2 1 1 2 1\n",
      " 1 2 2 2 2 1 1 2 2 1 2 1 2 1 2 2 0 1 2 2 1 2 2 1 1 0 1 2 1 1 1 2 2 2 2 1 1\n",
      " 1 2 2 2 2 1 2 2 1 1 2 2 2 2 1 1 2 2 1 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 2 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
      " 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(best_k)\n",
    "km.fit(tf_idf_array)\n",
    "print(km.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we haven't shuffled our dataset, we know that the first 100 documents belong in the first category, the next 100 in the second and so on. We can see that k-means did pretty well in clustering the documents.\n",
    "\n",
    "In a truly unsupervised problem we wouldn't have any means of evaluating the results of the clustering. One thing we can do is to print the top terms in each cluster. This way we can have an idea on the context of the documents in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: image thanks graphics hi know files program file card does mail information looking appreciated help use software images color vga\n",
      "Cluster 1: edu article baseball year don just game like team think better games know people good way time new really hitter\n",
      "Cluster 2: god edu think people atheism com exist article islam don believe does objective morality livesey moral true just islamic motto\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(best_k):\n",
    "    out = \"Cluster %d:\" % i\n",
    "    for ind in order_centroids[i, :20]:\n",
    "        out += ' %s' % terms[ind]\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we can get an intuition on what the documents of each cluster are about.\n",
    "\n",
    "We can even use a larger value for $k$ to see if there are any sub-categories in our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: islam jaeger bu islamic muslim rushdie gregg god edu muslims uk buphy law argument book article laws khomeini true women\n",
      "Cluster 1: year better hitter game team games good edu hit pitching baseball players season article like league com new braves play\n",
      "Cluster 2: thanks card does windows driver know runs hi vga advance viewer files postscript subject looking send graphics texas svga gl\n",
      "Cluster 3: god edu think atheism believe article exist universe don list said motto existence does com clemens going like bible question\n",
      "Cluster 4: morality keith livesey moral caltech jon com natural objective sgi edu people think don gifs wrong species article mil immoral\n",
      "Cluster 5: graphics mail newsgroup know just baseball info article edu week thanks comp hardware code lot group read email programmer databases\n",
      "Cluster 6: image program version file images files software use using thanks knows data hi ellipse tga problem code know called convert\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(8)\n",
    "km.fit(tf_idf_array)\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(7):\n",
    "    out = \"Cluster %d:\" % i\n",
    "    for ind in order_centroids[i, :20]:\n",
    "        out += ' %s' % terms[ind]\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks\n",
    "\n",
    "The methodologies we saw in this tutorial for converting natural language documents to a structured form, are called [Vector Space Model](https://en.wikipedia.org/wiki/Vector_space_model). The vector used to represent a document is called an **embedding**.\n",
    "\n",
    "One thing you should take away when dealing with these types of models, is that **reducing** the size of the vocabulary (i.e. the number of dimensions) with NLP methods (stemming/lemmatization, frequent/rarest term removal, etc.), usually outperforms doing the same thing with standard ML pre-processing (feature selection, PCA, etc.). Both methods should be used with a measure; you don't want to sacrifice any important features for the sake of dimensionality reduction.\n",
    "\n",
    "As a final word, unsupervised NLP problems can be **very** difficult. The categories we selected for this tutorial are easily distinguishable; a fact that helped us achieve a good performance. Don't expect this to always be the case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
